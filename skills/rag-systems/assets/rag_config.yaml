# RAG System Configuration
# Complete configuration for production RAG pipelines

# Document Processing
document_processing:
  # Supported file types
  file_types:
    - pdf
    - docx
    - txt
    - md
    - html
    - csv

  # Chunking configuration
  chunking:
    strategy: recursive  # fixed, sentence, paragraph, semantic, recursive
    chunk_size: 1000
    chunk_overlap: 200
    separators:
      - "\n\n"
      - "\n"
      - ". "
      - " "

  # Metadata extraction
  metadata:
    extract_title: true
    extract_dates: true
    extract_authors: true
    custom_fields:
      - document_type
      - source
      - version

# Embedding Configuration
embeddings:
  # Provider selection
  provider: openai  # openai, huggingface, cohere, local

  # Model settings
  models:
    openai:
      model: text-embedding-3-small
      dimensions: 1536
      batch_size: 100

    huggingface:
      model: sentence-transformers/all-MiniLM-L6-v2
      device: cuda
      normalize: true

    cohere:
      model: embed-english-v3.0
      input_type: search_document

    local:
      model_path: ./models/embeddings
      device: cuda

  # Caching
  cache:
    enabled: true
    backend: redis  # memory, redis, disk
    ttl: 86400  # 24 hours

# Vector Store Configuration
vector_store:
  provider: chroma  # chroma, pinecone, weaviate, milvus, qdrant, faiss

  # Provider-specific settings
  providers:
    chroma:
      persist_directory: ./chroma_db
      collection_name: documents

    pinecone:
      index_name: documents
      environment: us-west1-gcp
      metric: cosine
      replicas: 1
      shards: 1

    weaviate:
      url: http://localhost:8080
      class_name: Document
      text_key: content

    milvus:
      host: localhost
      port: 19530
      collection_name: documents
      index_type: IVF_FLAT
      nlist: 1024

    qdrant:
      url: http://localhost:6333
      collection_name: documents
      vector_size: 1536

    faiss:
      index_path: ./faiss_index
      index_type: IndexFlatIP

  # Index settings
  index:
    type: hnsw  # flat, ivf, hnsw
    parameters:
      m: 16
      ef_construction: 200
      ef_search: 100

# Retrieval Configuration
retrieval:
  # Search settings
  search:
    top_k: 10
    score_threshold: 0.7
    include_metadata: true

  # Hybrid search (dense + sparse)
  hybrid:
    enabled: true
    dense_weight: 0.7
    sparse_weight: 0.3
    sparse_model: bm25

  # Re-ranking
  reranking:
    enabled: true
    model: cross-encoder/ms-marco-MiniLM-L-6-v2
    top_k_rerank: 5

  # Query processing
  query_processing:
    expansion: true
    expansion_model: gpt-3.5-turbo
    max_expansions: 3

# Generation Configuration
generation:
  # LLM settings
  llm:
    provider: openai
    model: gpt-4
    temperature: 0.3
    max_tokens: 1000

  # Prompt template
  prompt_template: |
    Answer the question based on the following context.
    If you cannot answer from the context, say "I don't have enough information."

    Context:
    {context}

    Question: {question}

    Answer:

  # Response settings
  response:
    include_sources: true
    max_source_length: 500
    format: markdown

# Evaluation Metrics
evaluation:
  metrics:
    - faithfulness
    - answer_relevancy
    - context_precision
    - context_recall

  thresholds:
    faithfulness: 0.8
    relevancy: 0.7
    precision: 0.7
    recall: 0.7

# Monitoring
monitoring:
  logging:
    level: INFO
    log_queries: true
    log_retrievals: true

  metrics:
    enabled: true
    export_format: prometheus

  alerts:
    latency_threshold_ms: 2000
    error_rate_threshold: 0.05

# Performance Optimization
optimization:
  # Caching
  query_cache:
    enabled: true
    max_size: 10000
    ttl: 3600

  # Batching
  batch_processing:
    enabled: true
    batch_size: 50

  # Async processing
  async:
    enabled: true
    max_workers: 4

# Security
security:
  # Access control
  authentication: true
  rate_limiting:
    enabled: true
    requests_per_minute: 60

  # Data privacy
  pii_detection: true
  content_filtering: true
